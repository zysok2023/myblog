# 分布式数据库
**分布式数据库的核心为分片和同步**
## 数据分片
分片是将大数据表分解为较小的表（称为分片）的过程，这些分片分布在多个数据库集群节点上。分片本质上可以被看作传统数据库中的分区表，是一种水平扩展手段。每个分片上包含原有总数据集的一个子集，从而可以将总负载分散在各个分区之上。
数据分片的方式一般有两种。
- 水平分片：在不同的数据库节点中存储同一表的不同行
- 垂直分片：在不同的数据库节点中存储表不同的表列
## 分片算法
分片算法一般指代水平分片所需要的算法。经过多年的演化，其已经在大型系统中得到了广泛的实践。
### 哈希分片
哈希分片，首先需要获取分片键，然后根据特定的哈希算法计算它的哈希值，最后使用哈希值确定数据应被放置在哪个分片中。数据库一般对所有数据使用统一的哈希算法（例如 ketama），以促成哈希函数在服务器之间均匀地分配数据，从而降低了数据不均衡所带来的热点风险。通过这种方法，数据不太可能放在同一分片上，从而使数据被随机分散开。
这种算法非常适合随机读写的场景，能够很好地分散系统负载，但弊端是不利于范围扫描查询操作。下图是这一算法的工作原理。
### 范围分片
范围分片根据数据值或键空间的范围对数据进行划分，相邻的分片键更有可能落入相同的分片上。每行数据不像哈希分片那样需要进行转换，实际上它们只是简单地被分类到不同的分片上。
范围分片需要选择合适的分片键，这些分片键需要尽量不包含重复数值，也就是其候选数值尽可能地离散。同时数据不要单调递增或递减，否则，数据不能很好地在集群中离散，从而造成热点。范围分片非常适合进行范围查找，但是其随机读写性能偏弱。
### 融合算法
哈希和范围的分片算法并不是非此即彼，二选一的。相反，我们可以灵活地组合它们。可以建立一个多级分片策略，该策略在最上层使用哈希算法，而在每个基于哈希的分片单元中，数据将按顺序存储。
### 地理位置算法
该算法一般用于 NewSQL 数据库，提供全球范围内分布数据的能力。
在基于地理位置的分片算法中，数据被映射到特定的分片，而这些分片又被映射到特定区域以及这些区域中的节点。
然后在给定区域内，使用哈希或范围分片对数据进行分片。例如，在美国、中国和日本的 3 个区域中运行的集群可以依靠 User 表的 Country_Code 列，将特定用户（User）所在的数据行映射到符合位置就近规则的区域中。
### 手动分片 vs 自动分片
手动分片，顾名思义，就是设置静态规则来将数据根据分片算法分散到数据库节点。这一般是由于用户使用的数据库不支持自动的分片，如 MySQL、Oracle 等。这个问题可以在应用层面上做数据分片来解决，也可以使用简单的数据库中间件或 Proxy 来设置静态的分片规则来解决。
手动分片的缺点是数据分布不均匀。数据分布不均可能导致数据库负载极其不平衡，从而使其中一些节点过载，而另一些节点访问量较少。
因此，最好避免在部分节点上存储过多数据，否则会造成这些节点成为访问热点，进而导致其运行速度降低，甚至使服务器崩溃。此外，当整体数据集过小时，也会导致这个问题，因为集群中只有部分节点才有数据。

这在开发和测试环境中是可以接受的，但在生产环境中是不可以接受的。因为数据分布不均，热点以及将数据存储在太少的分片上，都会导致数据库集群内的节点计算资源耗尽，造成系统不稳定。
### 常见的分片算法
常用的算法有 UUID 和 Snowfalke 两种无状态生成算法。
- UUID
UUID 是最简单的方式，但是生成效率不高，且数据离散度一般。UUID（Universally Unique Identifier，通用唯一识别码）是一种用于标识信息的128位（16字节）的标识符。UUID的主要目的是在分布式系统中生成唯一的标识符，以确保每个标识符在全球范围内都是唯一的。UUID的标准格式通常为32个十六进制字符，由连字符（-）分隔成五组。UUID的组成规则如下：
版本（Version）：UUID的版本号决定了其生成算法。常见的版本有：
版本1（基于时间戳）：使用当前时间戳和机器的MAC地址来生成UUID。
版本2（基于时间和节点标识符）：与版本1类似，但包含一些额外的信息，如用户ID。
版本3（基于MD5散列）：使用MD5散列算法和命名空间标识符来生成UUID。
版本4（基于随机数或伪随机数）：使用随机数或伪随机数来生成UUID。
版本5（基于SHA-1散列）：使用SHA-1散列算法和命名空间标识符来生成UUID。
变体（Variant）：UUID的变体决定了其格式。常见的变体有：

变体0：遵循了最初的UUID规范。
变体1：遵循了DCE（分布式计算环境）规范。
变体2：遵循了Microsoft规范。
UUID的生成算法：

版本1：使用当前时间戳和机器的MAC地址。
版本2：与版本1类似，但包含一些额外的信息。
版本3：使用MD5散列算法和命名空间标识符。
版本4：使用随机数或伪随机数。
版本5：使用SHA-1散列算法和命名空间标识符。
UUID的生成算法和规则确保了每个UUID在全球范围内的唯一性，适用于各种分布式系统中的唯一标识需求。

UUID（Universally Unique Identifier）是一种标识符，用于在计算系统中唯一地标识实体。虽然UUID具有广泛的应用和许多优点，但也存在一些缺陷，包括：

可读性差：标准的UUID是一个由32个十六进制数字组成的字符串，例如："550e8400-e29b-41d4-a716-446655440000"。这使得它们在人类可读性和友好性方面较差，不适合直接用作用户界面或URL中的标识符。
占用空间：标准的UUID是128位长，通常以字符串形式表示。相比较于其他较短的标识符，如自增ID或短URL，UUID需要更多的存储空间。这在存储大量数据或在索引中使用UUID时可能会成为一个问题。
无序性：UUID是根据特定算法生成的，没有按照特定的顺序排序。这使得在数据库索引或范围查询时，UUID的无序性可能导致性能下降。相比之下，自增ID可以更容易地按照顺序进行查询和索引。
不适合作为随机数：尽管UUID是唯一的，但它们并不是真正的随机数。UUID的生成算法通常基于时间戳和机器标识等信息，这意味着在某些情况下，可能会存在可预测性或可推测性，尤其是在快速生成大量UUID时。
不适合某些特定需求：在某些特定的应用场景中，如高并发的分布式系统或特定的加密场景，UUID可能无法满足要求。例如，在某些分布式数据库系统中，需要更短、有序且易于分片的标识符，以提高性能和可扩展性。

- Snowfalke
Snowflake 是一种用于生成唯一ID的算法，由Twitter开发并开源。Snowflake算法生成的是一个64位的整数，由以下几个部分组成：

时间戳（41位）：表示当前时间相对于某个起始时间的毫秒数。由于时间戳是41位，这意味着Snowflake可以支持大约69年的时间（2^41 / (1000 * 60 * 60 * 24 * 365) ≈ 69年）。

数据中心ID（5位）：用于标识不同的数据中心。数据中心ID的范围是0到31，这意味着最多可以支持32个不同的数据中心。

机器ID（5位）：用于标识同一数据中心内的不同机器。机器ID的范围也是0到31，这意味着每个数据中心最多可以支持32台机器。

序列号（12位）：用于在同一毫秒内生成多个ID。序列号的范围是0到4095，这意味着在同一毫秒内最多可以生成4096个不同的ID。

第一个部分是时间戳，占41位。
第二个部分是数据中心ID，占5位。
第三个部分是机器ID，占5位。
第四个部分是序列号，占12位。
Snowflake算法的优点是：

生成的ID是递增的，这对于数据库索引和排序非常有用。
生成的ID是唯一的，即使在分布式系统中也能保证唯一性。
生成的ID是可排序的，可以根据时间戳进行排序。
Snowflake算法的缺点是：

生成的ID长度较长，占用更多的存储空间。
需要配置和管理数据中心ID和机器ID，增加了系统的复杂性。
总的来说，Snowflake算法是一种高效、唯一且可排序的ID生成算法，适用于需要生成大量唯一ID的分布式系统。

### 灵活的分片算法
为了保证分片计算的灵活性，ShardingShpere 提供了标准分片算法和一些工具，帮助用户实现个性化算法。
- PreciseShardingAlgorithm 配合哈希函数使用，可以实现哈希分片。RangeShardingAlogrithm 可以实现范围分片。
- 使用 ComplexShardingStrategy 可以使用多个分片键来实现融合分片算法。
- 有的时候，数据表的分片模式不是完全一致。对于一些特别的分片模式，可以使用 HintShardingStrategy 在运行态制定特殊的路由规则，而不必使用统一的分片配置。
- 如果用户希望实现诸如地理位置算法等特殊的分片算法，可以自定义分片策略。使用 inline 表达式或 Java 代码进行编写，前者基于配置不需要编译，适合简单的个性化分片计算；后者可以实现更加复杂的计算，但需要编译打包的过程。

ShardingShpere 提供了 Sharding-Scale 来支持数据库节点弹性伸缩，该功能就是其对自动分片的支持。
自动分片包含下图所示的四个过程：
![自动分片](https://cdn.jsdelivr.net/gh/zysok2023/cloudImg/blogs/picture/自动分片.png)

## 数据复制
复制的主要目的是在几个不同的数据库节点上保留相同数据的副本，从而提供一种数据冗余。这份冗余的数据可以提高数据查询性能，而更重要的是保证数据库的可用性。

### 单主复制
单主复制，也称主从复制。写入主节点的数据都需要复制到从节点，即存储数据库副本的节点。当客户要写入数据库时，他们必须将请求发送给主节点，而后主节点将这些数据转换为复制日志或修改数据流发送给其所有从节点。从使用者的角度来看，从节点都是只读的。下图就是经典的主从复制架构。
![单主复制](https://cdn.jsdelivr.net/gh/zysok2023/cloudImg/blogs/picture/单主复制.png)

这种模式是最早发展起来的复制模式，不仅被广泛应用在传统数据库中，如 PostgreSQL、MySQL、Oracle、SQL Server；它也被广泛应用在一些分布式数据库中，如 MongoDB、RethinkDB 和 Redis 等。
- 复制同步模式
复制是一个非常耗费时间而且很难预测完成情况的操作。虽然其受影响的因素众多，但一个复制操作是同步发生还是异步发生，被认为是极为重要的影响因素，可以从以下三点来分析。
1. 同步复制：如果由于从库已崩溃，存在网络故障或其他原因而没有响应，则主库也无法写入该数据。
2. 异步复制：如果由于从库已崩溃，存在网络故障或其他原因而没有响应，则主库可以继续写入数据。
3. 半同步复制：如果由于从库已崩溃，存在网络故障或其他原因而没有响应，则主库无法写入该数据，直到从库恢复并响应为止。
可以看到不同的同步模式是在性能和一致性上做平衡，三种模式对应不同场景，并没有好坏差异。用户需要根据自己的业务场景来设置不同的同步模式。
- 复制延迟
复制延迟是指从主库到从库的数据复制时间。复制延迟是影响数据库性能和可用性的一个重要因素。如果复制延迟过大，可能会导致从库的数据与主库的数据不一致，从而影响数据库的可用性和一致性。
- 复制与高可用性
高可用（High availablity）是一个 IT 术语，指系统无中断地执行其功能的能力。系统中的任何节点都可能由于各种出其不意的故障而造成计划外停机；同时为了要维护系统，我们也需要一些计划内的停机。采用主从模式的数据库，可以防止单一节点挂起导致的可用性降低的问题。
系统可用程度一般使用小数点后面多个 9 的形式，如下表所示。
|可用性	|年故障时间|
|--|--|
|99.9999%|32秒|
|99.999%|5分15秒|
|99.99%	|52分34秒|
|99.9%	|8小时46分|
|99%	|3天15小时36分|
一般的生产系统都会至少有两个9的保证，追求三个9。想要做到4个9是非常最具有挑战的。
在主从模式下，为了支撑高可用，就需要进行故障处理:
1.从节点故障。由于每个节点都复制了从主库那里收到的数据更改日志，因此它知道在发生故障之前已处理的最后一个事务，由此可以凭借此信息从主节点或其他从节点那里恢复自己的数据。
2.主节点故障。在这种情况下，需要在从节点中选择一个成为新的主节点，此过程称为故障转移，可以手动或自动触发。其典型过程为：第一步根据超时时间确定主节点离线；第二步选择新的主节点，这里注意新的主节点通常应该与旧的主节点数据最为接近；第三步是重置系统，让它成为新的主节点。
### 复制方式
- 基于语句的复制
主库记录它所执行的每个写请求（一般以 SQL 语句形式保存），每个从库解析并执行该语句，就像从客户端收到该语句一样。但这种复制会有一些潜在问题，如语句使用了获取当前时间的函数，复制后会在不同数据节点上产生不同的值。
另外如自增列、触发器、存储过程和函数都可能在复制后产生意想不到的问题。但可以通过预处理规避这些问题。使用该复制方式的分布式数据库有 VoltDB、Calvin。
- 日志（WAL）同步
WAL 是一组字节序列，其中包含对数据库的所有写操作。它的内容是一组低级操作，如向磁盘的某个页面的某个数据块写入一段二进制数据，主库通过网络将这样的数据发送给从库。这种方法避免了上面提到的语句中部分操作复制后产生的一些副作用，但要求主从的数据库引擎完全一致，最好版本也要一致。如果要升级从库版本，那么就需要计划外停机。PostgreSQL 和 Oracle 中使用了此方法。
- 行复制
它由一系列记录组成，这些记录描述了以行的粒度对数据库表进行的写操作。它与特定存储引擎解耦，并且第三方应用可以很容易解析其数据格式。
### 多主复制
也称为主主复制。数据库集群内存在多个对等的主节点，它们可以同时接受写入。每个主节点同时充当主节点的从节点。
多主节点的架构模式最早来源于 DistributedSQL 这一类多数据中心，跨地域的分布式数据库。在这样的物理空间相距甚远，有多个数据中心参与的集群中，每个数据中心内都有一个主节点。而在每个数据中心的内部，却是采用常规的单主复制模式。
这么设计该类系统的目的在于以下几点。
1. 获得更好的写入性能：使数据可以就近写入。
2. 数据中心级别的高可用：每个数据中心可以独立于其他数据中心继续运行。
3. 更好的数据访问性能：用户可以访问到距离他最近的数据中心。
但是，此方法的最大缺点是，存在一种可能性，即两个不同的主节点同时修改相同的数据。这其实是非常危险的操作，应尽可能避免。这就需要下一讲要介绍的一致性模型，配合冲突解决机制来规避。
典型的多主复制产品有 MySQL 的 Tungsten Replicator、PostgreSQL 的 BDR 和 Oracle 的 GoldenGate。
目前，大部分 NewSQL、DistributedSQL 的分布式数据库都支持多主复制，但是大部分是用 Paxos 或 Raft 等协议来构建复制组，保证写入线性一致或顺序一致性；同时传统数据库如 MySQL 的 MGR 方案也是使用类似的方式，可以看到该方案是多主复制的发展方向。

### MySQL 复制技术的发展
MySQL 由于其单机机能的限制，很早就发展了数据复制技术以提高性能。同时依赖该技术，MySQL 可用性也得到了长足的发展。

截止到现在，该技术经历了四代的发展。第一代为传统复制，使用 MHA（Master High Available）架构；第二代是基于 GTID 的复制，即 GTID+Binlog server 的模式；第三代为增强半同步复制，GTID+增强半同步复制；第四代为 MySQL 原生高可用，即 MySQL InnoDB Cluster。
数据库的复制技术需要考虑两个因素：数据一致 RPO 和业务连续性 RTO。所以，就像前面的内容所强调的，复制与一致性是一对如影随形的概念。
- MHA 复制控制
![MHA架构](https://cdn.jsdelivr.net/gh/zysok2023/cloudImg/blogs/picture/MHA架构.png)
MHA 作为第一代复制架构，有如下适用场景：
1. MySQL 的版本≤5.5，这一点说明它很古老
2. 只用于异步复制且一主多从环境；
3. 基于传统复制的高可用。
- 半同步复制
这是第二代复制技术，它与第一代技术的差别表现在以下几点。
1. binlog 使用半同步，而第一代是异步同步。它保障了数据安全，一般至少要同步两个节点，保证数据的 RPO。
2. 同时保留异步复制，保障了复制性能。并通过监控复制的延迟，保证了 RTO。
3. 引入配置中心，如 consul。对外提供健康的 MySQL 服务。
4. 这一代开始需要支持跨 IDC 复制。需要引入监控 Monitor，配合 consul 注册中心。多个 IDC 中 Monitor 组成分布式监控，把健康的 MySQL 注册到 consul 中，同时将从库复制延迟情况也同步到consul中。
第二代复制技术也有自身的一些缺陷。
1. 存在幻读的情况。当事务同步到从库但没有 ACK 时，主库发生宕机；此时主库没有该事务，而从库有。
2. MySQL 5.6 本身半同步 ACK 确认在 dump_thread 中，dump_thread 存在 IO 瓶颈问题。
- 增强半同步复制
这一代需要 MySQL 是 5.7 以后的版本。有一些典型的框架来支持该技术，如 MySQL Replication Manager、GitHub-orchestrator 和国内青云开源的 Xenon 等。
这一代复制技术采用的是增强半同步。首先主从的复制都是用独立的线程来运行；其次主库采用 binlog group commit，也就是组提交来提供数据库的写入性能；而从库采用并行复制，它是基于事务的，通过数据参数调整线程数量来提高性能。这样主库可以并行，从库也可以并行。

这一代技术体系强依赖于增强半同步，利用半同步保证 RPO，对于 RTO，则取决于复制延迟。
第三代技术也有自身的缺点，如增强半同步中存在幽灵事务。这是由于数据写入 binlog 后，主库掉电。由于故障恢复流程需要从 binlog 中恢复，那么这份数据就在主库。但是如果它没有被同步到从库，就会造成从库不能切换为主库，只能去尝试恢复原崩溃的主库。
- MySQL 组复制
组复制是 MySQL 提供的新一代高可用技术的重要组成。其搭配 MySQL Router 或 Proxy，可以实现原生的高可用。
从这一代开始，MySQL 支持多主复制，同时保留单主复制的功能。其单主高可用的原理与第三代技术类似。
现在说一下它的多主模式，原理是使用 MySQL Router 作为数据路由层，来控制读写分离。而后组内部使用 Paxos 算法构建一致性写入。
它与第三代复制技术中使用的一致性算法的作用不同。三代中我们只使用该算法来进行选主操作，数据的写入并不包含在其中；而组复制的多主技术需要 Paxos 算法深度参与，并去决定每一次数据的写入，解决写入冲突。
组复制有如下几个优点。
- 高可用分片：数据库节点动态添加和移除。分片实现写扩展，每个分片是一个复制组。可以结合上一讲中对于 TiDB 的介绍，原理类似。
- 自动化故障检测与容错：如果一个节点无法响应，组内大多数成员认为该节点已不正常，则自动隔离。
- 方案完整：前面介绍的方案都需要 MySQL 去搭配一系列第三方解决方案；而组复制是原生的完整方案，不需要第三方组件接入。
当然，组复制同样也有一些限制。主要集中在需要使用较新的特性，一些功能在多组复制中不支持，还有运维人员经验缺乏等。

## CAP理论
### 基础概念
CAP理论是分布式系统、特别是分布式存储领域中被讨论的最多的理论。其中C代表一致性 (Consistency)，A代表可用性 (Availability)，P代表分区容错性 (Partition tolerance)。CAP理论告诉我们C、A、P三者不能同时满足，最多只能满足其中两个。

![CAP理论](https://cdn.jsdelivr.net/gh/zysok2023/cloudImg/blogs/picture/CAP理论.png)

一致性 (Consistency): 一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据。所有节点访问同一份最新的数据。
可用性 (Availability): 对数据更新具备高可用性，请求能够及时处理，不会一直等待，即使出现节点失效。
分区容错性 (Partition tolerance): 能容忍网络分区，在网络断开的情况下，被分隔的节点仍能正常对外提供服务。
对于一个分布式数据存储系统来说，如果没有网络故障，那么CAP的 三个特性都是可以满足的。

### CAP模型
- CP模型
在CP模型下，因为要满足C的一致性，所以一旦网络出现问题导致数据同步失败，此时数据的读取就会被拒绝从而导致读取超时或失败，这种情况下，系统变得不可用，即A不满足。
- AP模型
在AP模型下，因为要满足A的可用性，所以就算网络出现问题导致数据同步失败，此时数据的读取还是能够成功读取到数据，但这种情况下节点间的数据是不同的，即C不满足。

### 选择
![Nacos 服务发现产品对比](https://cdn.jsdelivr.net/gh/zysok2023/cloudImg/blogs/picture/Nacos 服务发现产品对比.png)

1. 服务注册中心，是选择AP还是选择CP ？
服务注册中心主要是解决什么问题：一个是服务注册，一个是服务发现。
- 服务注册：实例将自身服务信息注册到注册中心，这部分信息包括服务的主机IP和服务的Port，以及暴露服务自身状态和访问协议信息等。
- 服务发现：实例请求注册中心所依赖的服务信息，服务实例通过注册中心，获取到注册到其中的服务实例的信息，通过这些信息去请求它们提供的服务。
目前作为注册中心的一些组件大致有：dubbo的zookeeper，springcloud的eureka，consul，rocketMq的nameServer，hdfs的nameNode，阿里的nacos。目前微服务主流是dubbo和springcloud，使用最多是zookeeper和eureka，我们就来看看应该根据CAP理论应该怎么去选择注册中心。

zookeeper选择CP
zookeep保证CP，即任何时刻对zookeeper的访问请求能得到一致性的数据结果，同时系统对网络分割具备容错性，但是它不能保证每次服务的可用性。从实际情况来分析，在使用zookeeper获取服务列表时，如果zk正在选举或者zk集群中半数以上的机器不可用，那么将无法获取数据。所以说，zk不能保证服务可用性。

eureka选择AP
eureka保证AP，eureka在设计时优先保证可用性，每一个节点都是平等的，一部分节点挂掉不会影响到正常节点的工作，不会出现类似zk的选举leader的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点，只要有一台eureka存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。


对于服务注册来说，针对同一个服务，即使注册中心的不同节点保存的服务注册信息不相同，也并不会造成灾难性的后果，对于服务消费者来说，能消费才是最重要的，就算拿到的数据不是最新的数据，消费者本身也可以进行尝试失败重试。总比为了追求数据的一致性而获取不到实例信息整个服务不可用要好。所以，对于服务注册来说，可用性比数据一致性更加的重要，选择AP。
2. 分布式锁，是选择AP还是选择CP ？
- 基于数据库实现分布式锁
利用表的 UNIQUE KEY idx_lock (method_lock) 作为唯一主键，当进行上锁时进行insert动作，数据库成功录入则以为上锁成功，当数据库报出 Duplicate entry 则表示无法获取该锁。
- 基于redis实现分布式锁
redis单线程串行处理天然就是解决串行化问题，用来解决分布式锁是再适合不过。
为了解决数据库锁的无主从切换的问题，可以选择redis集群，或者是 sentinel 哨兵模式，实现主从故障转移，当master节点出现故障，哨兵会从slave中选取节点，重新变成新的master节点。
哨兵模式故障转移是由sentinel集群进行监控判断，当maser出现异常即复制中止，重新推选新slave成为master，sentinel在重新进行选举并不在意主从数据是否复制完毕具备一致性。
所以redis的复制模式是属于AP的模式。保证可用性，在主从复制中“主”有数据，但是可能“从”还没有数据，这个时候，一旦主挂掉或者网络抖动等各种原因，可能会切换到“从”节点，这个时候可能会导致两个业务线程同时获取得两把锁
![redis的复制模式](https://cdn.jsdelivr.net/gh/zysok2023/cloudImg/blogs/picture/redis的复制模式.png)
能不能使用redis作为分布式锁，这个本身就不是redis的问题，还是取决于业务场景，我们先要自己确认我们的场景是适合 AP 还是 CP ， 如果在社交发帖等场景下，我们并没有非常强的事务一致性问题，redis提供给我们高性能的AP模型是非常适合的，但如果是交易类型，对数据一致性非常敏感的场景，我们可能要寻在一种更加适合的 CP 模型

- 基于zookeeper实现分布式锁
首先zk的模式是CP模型，也就是说，当zk锁提供给我们进行访问的时候，在zk集群中能确保这把锁在zk的每一个节点都存在。
无论是redis，zk，例如redis的AP模型会限制很多使用场景，但它却拥有了几者中最高的性能，zookeeper的分布式锁要比redis可靠很多，但他繁琐的实现机制导致了它的性能不如redis，而且zk会随着集群的扩大而性能更加下降。

## BASE理论
BASE模型是传统ACID模型的反面，不同与ACID，BASE强调牺牲高一致性，从而获得可用性，数据允许在一段时间内的不一致，只要保证最终一致就可以了。

- Basically Available（基本可用）分布式系统在出现不可预知故障的时候，允许损失部分可用性
- Soft state（软状态）软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
- Eventually consistent（最终一致性）最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。
BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），更具体地说，是对 CAP 中 AP 方案的一个补充。其基本思路就是：通过业务，牺牲强一致性而获得可用性，并允许数据在一段时间内是不一致的，但是最终达到一致性状态。








































